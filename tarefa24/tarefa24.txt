Computacao Paralela
Ana Laura Fernandes de Oliveira
Larissa Domingues Gomes
Pedro Henrique Lima Carvalho
Pedro Henrique Reis Rodrigues
Tárcila Fernanda Resende da Silva


O gasto maior de tempo visto na execução do código em cuda sem memória local compartilhada se dá
em razão do uso do próprio arranjo dos números a ser somado, memória global, para ler e armazenar 
as somas parciais de cada bloco, o que é um pouco mais lento. No código com memória local, para
cada bloco, não há necessidade, dentro do processamento de cada bloco, de acesso a memória global.
No entanto, o problema de desempenho da solução com memória local é a necessidade de sincronização
das threads. Serão sincronizadas após a cópia dos elementos para a memória local de cada bloco e
novamente depois dentro de cada etapa da soma. No código em cuda sem memória local, somente é 
necessária a sincronização em cada etapa de soma. Por isso a diferença dos códigos em cuda com
e sem memória local não é acentuada.
Pelo tempo gasto pela função "CUDA memcpy HtoD" em comparação com o tempo da função sum_cuda 
é possível verificar que o overhead para transferência dos dados para a GPU é considerável em
comparação com o tempo de computação efetivo. Isso explica porque os tempos serial e multicore
foram melhores. De fato, a GPU leva apenas 21 ms aproximadamente para as somas, ao contrário da 
CPU que leva em torno de 400ms. Mas o overhead de cópia da CPU para a GPU e vice-versa, prejudica 
o desempenho final.
Por fim, o desempenho do código com uso de GPU com openmp teve desempenho similar aos em cuda, 
confirmando que o overhead do uso da GPU prejudica bastante o desempenho do código.


Código sum_cuda.cu -> Fornecido

Sum = 799999980000000.000000

real	0m1.511s
user	0m0.497s
sys	0m0.930s

==14204== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 95.47%  463.12ms         1  463.12ms  463.12ms  463.12ms  [CUDA memcpy HtoD]
  4.45%  21.595ms         1  21.595ms  21.595ms  21.595ms  sum_cuda(double*, double*, int)
  0.08%  367.69us         1  367.69us  367.69us  367.69us  [CUDA memcpy DtoH]



Código sum_serial -> Sequencial

Sum = 799999980000000.000000

real	0m0.420s
user	0m0.119s
sys	0m0.296s


Código sum_multicore -> Paralelo Multicore

Sum = 799999980000000.000000

real	0m0.313s
user	0m0.235s
sys	0m0.227s


Código sum_gpu -> Paralelo GPU Openmp

Sum = 799999980000000.000000

real	0m1.931s
user	0m0.803s
sys	0m1.032s

==1692== Profiling application: ./a.out
==1692== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 54.80%  468.11ms         5  93.622ms     832ns  468.11ms  [CUDA memcpy HtoD]
 44.33%  378.65ms         2  189.32ms  1.9520us  378.65ms  [CUDA memcpy DtoH]
  0.86%  7.3869ms         1  7.3869ms  7.3869ms  7.3869ms  main$_omp_fn$0



Código sum_noshared_cuda -> Cuda GPU sem memória compartilhada

Sum = 799999980000000.000000

real	0m1.884s
user	0m0.841s
sys	0m0.927s

==2168== Profiling application: ./a.out
==2168== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 93.97%  468.77ms         1  468.77ms  468.77ms  468.77ms  [CUDA memcpy HtoD]
  5.96%  29.744ms         1  29.744ms  29.744ms  29.744ms  sum_cuda(double*, double*, int)
  0.07%  362.03us         1  362.03us  362.03us  362.03us  [CUDA memcpy DtoH]


